# Semantic Kernel Environment Variables
# This file contains environment variables for various AI service providers used with Semantic Kernel

# ==========================================
# OpenAI Configuration
# ==========================================
OPENAI_API_KEY=your-openai-api-key
OPENAI_ORG_ID=your-organization-id  # Optional
OPENAI_CHAT_MODEL_ID=gpt-4-turbo-preview
OPENAI_TEXT_MODEL_ID=gpt-3.5-turbo-instruct  # For text completion if needed

# ==========================================
# Azure OpenAI Configuration
# ==========================================
AZURE_OPENAI_API_KEY=your-azure-openai-api-key
AZURE_OPENAI_ENDPOINT=https://your-resource-name.openai.azure.com/
AZURE_OPENAI_DEPLOYMENT_NAME=your-deployment-name
AZURE_OPENAI_API_VERSION=2023-09-01-preview  # Use the latest API version

# ==========================================
# Azure AI Inference Configuration
# ==========================================
AZURE_AI_INFERENCE_ENDPOINT=https://your-endpoint.inference.ai.azure.com/
AZURE_AI_INFERENCE_API_KEY=your-azure-ai-inference-api-key
AZURE_AI_INFERENCE_MODEL_ID=your-model-id  # Only needed for multi-model endpoints

# ==========================================
# Anthropic Configuration
# ==========================================
ANTHROPIC_API_KEY=your-anthropic-api-key
ANTHROPIC_MODEL_ID=claude-3-opus-20240229  # Use the Claude model you prefer

# ==========================================
# AWS Bedrock Configuration
# ==========================================
# AWS credentials (If not using AWS CLI profile)
AWS_ACCESS_KEY_ID=your-aws-access-key
AWS_SECRET_ACCESS_KEY=your-aws-secret-key
AWS_REGION=us-west-2  # Your AWS region where Bedrock is available
BEDROCK_MODEL_ID=anthropic.claude-3-sonnet-20240229-v1:0  # Update with your preferred model

# ==========================================
# Google AI Configuration
# ==========================================
GOOGLE_API_KEY=your-google-ai-api-key
GOOGLE_AI_MODEL=gemini-pro  # Your preferred Google AI model

# ==========================================
# MistralAI Configuration
# ==========================================
MISTRAL_API_KEY=your-mistral-api-key
MISTRAL_MODEL_ID=mistral-large-latest  # Your preferred Mistral model

# ==========================================
# Ollama Configuration
# ==========================================
OLLAMA_API_BASE=http://localhost:11434  # Default Ollama API endpoint
OPENAI_CHAT_MODEL_ID=llama3  # Your preferred Ollama model

# ==========================================
# ONNX Configuration
# ==========================================
# Typically ONNX runs locally and may not need API credentials
# But it might need model paths
ONNX_MODEL_PATH=/path/to/your/onnx/model  # Optional: Path to your ONNX model

# ==========================================
# Vertex AI Configuration
# ==========================================
# For authentication with service account key
GOOGLE_APPLICATION_CREDENTIALS=/path/to/your/service-account-key.json
VERTEX_AI_PROJECT_ID=your-gcp-project-id
VERTEX_AI_LOCATION=us-central1  # Your GCP region
VERTEX_AI_MODEL_ID=gemini-pro  # Your preferred Vertex AI model

# ==========================================
# DeepSeek Configuration
# ==========================================
# DeepSeek uses OpenAI-compatible API
# Uses OPENAI_API_KEY but with a different base URL (handled in code)
DEEPSEEK_API_KEY=your-deepseek-api-key  # This can be used as OPENAI_API_KEY when using DeepSeek
DEEPSEEK_MODEL_ID=deepseek-chat  # or deepseek-reasoner

# ==========================================
# MCP Server Configuration (if needed)
# ==========================================
MCP_CONFIG_PATH=./mcp.json  # Optional: path to your MCP configuration file